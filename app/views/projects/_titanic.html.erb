<p class="firstParagraph"
   xmlns="http://www.w3.org/1999/html">The Titanic shipwreck led to the death of 1502 out of 2224 passengers in 1912. One of the reasons of such loss of life was the insufficient supply of lifeboats. This project predicts what sort of people are likely to survive. The evaluation criteria is the percentage of correct predictions.</p>

<h3>0. Data</h3>
The data contains the following variables:
<pre class="dataPre">
survival        Survival (0 = No; 1 = Yes)
pclass          Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)
name            Name
sex             Sex
age             Age
sibsp           Number of Siblings/Spouses Aboard
parch           Number of Parents/Children Aboard
ticket          Ticket Number
fare            Passenger Fare
cabin           Cabin
embarked        Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)
</pre>

Here's a snippet of the data:

<table class="table table-bordered table-hover table-condensed">
  <thead>
      <tr>
        <th>survived</th>
        <th>pclass</th>
        <th>name</th>
        <th>sex</th>
        <th>age</th>
        <th>sibsp</th>
        <th>parch</th>
        <th>ticket</th>
        <th>fare</th>
        <th>cabin</th>
        <th>embarked</th>
      </tr>
  </thead>
  <tbody>
      <tr>
        <td>0</td>
        <td>3</td>
        <td>Braund, Mr. Owen Harris</td>
        <td>male</td>
        <td>22</td>
        <td>1</td>
        <td>0</td>
        <td>A/5 21171</td>
        <td>7.25</td>
        <td></td>
        <td>S</td>
      </tr>
      <tr>
        <td>0</td>
        <td>1</td>
        <td>Van der hoef, Mr. Wyckoff</td>
        <td>male</td>
        <td>61</td>
        <td>0</td>
        <td>0</td>
        <td>111240</td>
        <td>33.5</td>
        <td>B19</td>
        <td>S</td>
      </tr>
      <tr>
        <td>1</td>
        <td>2</td>
        <td>Watt, Mrs. James (Elizabeth "Bessie" Inglis Milne)</td>
        <td>female</td>
        <td>40</td>
        <td>0</td>
        <td>0</td>
        <td>C.A. 33595</td>
        <td>15.75</td>
        <td></td>
        <td>S</td>
      </tr>
      <tr>
        <td>1</td>
        <td>3</td>
        <td>Goldsmith, Master. Frank John William "Frankie"</td>
        <td>male</td>
        <td>9</td>
        <td>0</td>
        <td>2</td>
        <td>363291</td>
        <td>20.525</td>
        <td>A7</td>
        <td>Q</td>
      </tr>
  </tbody>
</table>

We can see that the data isn't ideal (i.e. ready to be dumped into all major Machine Learning algorithms). For example, there are:
<ul>
  <li>Attributes with missing data (e.g. <i>cabin</i> as shown above). Specifically, the <i>age</i>, <i>cabin</i> and <i>embarked</i> attributes have 177 (19.9%), 687 (77%) and 2 (0.1%) pieces of missing data, respective; for a total of 866 (9.7% overall) missing slots. Now that is a lot of missing data -- it almost renders the <i>cabin</i> attribute too sparse to be useful.</li>
  <li>The <i>name</i>, <i>cabin</i> and <i>ticket</i> attributes seem too versatile to be useful.</li>
</ul>

<h4>0.1 Data Modification</h4>
Some data fields need to be mangled before they can be plugged into machine learning algorithms.
<ul>
  <li>
    <i>Name</i> is too important to be omitted. So what useful information can we extract from it? The title -- Master, Mr, Mrs, Miss, etc. It provides important information about an individual's social status, marital status, gender and vaguely the age. It is especially helpful when <i>age</i> and <i>sex</i> fields have missing data.
  </li>
  <li>
    <i>Cabin</i> comes in the format of "C85", "A6", "B78", etc. The first letter, which represents the main cabin section, is preserved; the numbers are omitted.
  </li>
  <li>
    <i>Ticket</i> is difficult to work with because it consists of a wide range of letters and numbers, e.g. "STON/O2. 3101282", "17463", "PC 17601", etc. The letters are kept. The numbers are discretized by preserving the first digit and the number of digits, e.g. "17463" becomes "1_5".
  </li>
  <li>
    Missing data could be filled by the average or the mode of the available data, or simply a fixed value. As we'll see in a later section, determining how missing data should be filled is in fact part of the parameters-tuning process. In addition, <i>age</i> is so important that we fill missing values with linear SVM predictions made using the other fields.
  </li>
</ul>

<h4>0.2 Feature Importance</h4>
<p>Since there are only 10 fields, feature selection is unnecessary. However, it is still interesting and useful to know which features are more important.</p>
<p>First rank the attributes according to their Random Forest (100 trees) importance, linear SVM weights, Information Gain, Gain Ratio, and Gini index, sorted by Random Forest importance (shown in table below).</p>
<table class="table table-bordered table-hover table-condensed">
  <thead>
  <tr>
    <td>Attribute</td>
    <td>RF</td>
    <td>SVM</td>
    <td>Inf. Gain</td>
    <td>Gain Ratio</td>
    <td>Gini</td>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>sex</td>
    <td>0.242</td>
    <td>1.402</td>
    <td>0.218</td>
    <td>0.232</td>
    <td>0.070</td>
  </tr>
  <tr>
    <td>age</td>
    <td>0.179</td>
    <td>0.432</td>
    <td>0.016</td>
    <td>0.005</td>
    <td>0.005</td>
  </tr>
  <tr>
    <td>fare</td>
    <td>0.153</td>
    <td>0.192</td>
    <td>0.099</td>
    <td>0.030</td>
    <td>0.031</td>
  </tr>
  <tr>
    <td>pclass</td>
    <td>0.090</td>
    <td>0.139</td>
    <td>0.084</td>
    <td>0.058</td>
    <td>0.027</td>
  </tr>
  <tr>
    <td>name</td>
    <td>0.084</td>
    <td>0.269</td>
    <td>0.239</td>
    <td>0.144</td>
    <td>0.075</td>
  </tr>
  <tr>
    <td>ticket</td>
    <td>0.080</td>
    <td>0.179</td>
    <td>0.147</td>
    <td>0.037</td>
    <td>0.043</td>
  </tr>
  <tr>
    <td>cabin</td>
    <td>0.064</td>
    <td>0.032</td>
    <td>0.022</td>
    <td>0.009</td>
    <td>0.007</td>
  </tr>
  <tr>
    <td>sibsp</td>
    <td>0.050</td>
    <td>0.362</td>
    <td>0.033</td>
    <td>0.025</td>
    <td>0.010</td>
  </tr>
  <tr>
    <td>embarked</td>
    <td>0.030</td>
    <td>0.006</td>
    <td>0.021</td>
    <td>0.019</td>
    <td>0.007</td>
  </tr>
  <tr>
    <td>parch</td>
    <td>0.030</td>
    <td>0.257</td>
    <td>0.024</td>
    <td>0.021</td>
    <td>0.007</td>
  </tr>
  </tbody>
</table>
<p>Below is a plot of average feature importances with their respective standard deviations calculated from a random forest with 100 trees.</p>
<%= image_tag 'titanic/feature importances.png' %>

<h4>0.3 Data Visualization</h4>
<h3>1. Preliminary Fitting</h3>
Let's ignore the fact that the data isn't pretty and feature selection has yet to be done, and just first fit a model for benchmarking.
<h3>Feature Selection</h3>

<h3>2. Where do we go from here?</h3>

<h3>3. Conclusions</h3>
